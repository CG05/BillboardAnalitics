{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 크롤링"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import urllib.request as req\n",
    "import urllib.parse as par\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "\n",
    "# URL\n",
    "base_url = \"https://www.inven.co.kr/\"\n",
    "url = base_url + \"board/lostark/6271?my=chu\"\n",
    "\n",
    "# URL Open\n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup\n",
    "soup = BeautifulSoup(res, \"html.parser\")\n",
    "\n",
    "# News link \n",
    "boardlist = soup.select(\"#new-board > form > div > table > tbody > tr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:6: SyntaxWarning: invalid escape sequence '\\['\n",
      "<>:6: SyntaxWarning: invalid escape sequence '\\['\n",
      "C:\\Users\\PC\\AppData\\Local\\Temp\\ipykernel_13020\\838683667.py:6: SyntaxWarning: invalid escape sequence '\\['\n",
      "  title = re.sub('(\\[.*\\])', '', title, 0).strip()\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def exceptTag(title):\n",
    "    title = re.sub('<.+?>', '', str(title), 0).strip()\n",
    "    # [] 이후~ 중요 문장 이전 공백 제거\n",
    "    title = re.sub('(\\[.*\\])', '', title, 0).strip()\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ㅈ같아서 고정 폭파하고 접는다\n",
      "메이플에 완패한 로스트아크...jpg\n",
      "이제 로아에도 자리 문화 생김?\n",
      "ㅇㅂ) 인기 아크패시브 공개하고 인벤에 살인스텝만 당했다\n",
      "끼순이 전재학.avi\n",
      "눈천지 신도들 명단 100명 가득채웠다\n",
      "스마게 지피방,원격피방 다시 잡기 시작함.\n",
      "현 노멘 상황 ㅋㅋ\n",
      "팩트는 세우라제가 개병신으로 너프먹고 뒤지는게 로아가 건강해지는 길임\n",
      "아게오로스 매칭 절대 가지마셈\n",
      "ㅎㅂ) 레지던트 이블에 진출한 에키드나\n",
      "인방x) 아르고스 근황\n",
      "fact)스커새끼들 요구하는게 양심없는 이유\n",
      "요새 한국이 너무 흉흉하네요.JPG\n",
      "30추글간 천만원 딜빵 본인입니다.\n",
      "블래스터 아크패시브 공개...gif\n",
      "포피셜) 모든 강화 효율 정리표\n",
      "천만골드 딜빵 하기로 했는데 증인해주세요.\n",
      "전재학 디렉터님께.\n",
      "로아에는 낭만을 아는 658명이 있다.\n",
      "ㅎㅂ) ㅇㄷㅇ\n",
      "ㅇㅎ)전설의 맥심 에디터\n",
      "카멘의 구슬 아이스크림.gif\n",
      "서폿 인구 존나 줄어든건 인벤탓이 아니다.\n",
      "아이 개씨발새끼들아 눈가루 관심없다고\n",
      "현시점 눈가루보다 ㅈ된 새끼ㅋㅋㅋ\n",
      "ㅇㅂ?) 로드나인 가장 치명적인 단점..\n",
      "고고학 60만생기 녹이고 결론 내린 삽 효율\n",
      "ㅇㅂ) 눈가루 당사자와 dm\n",
      "같이...디코해주던...애가...왜우냐고 물어....\n",
      "남성 비하 손가락 동작 들어간 이마트24 영상 논란\n",
      "우리집엔 귀여운 포메가 살아\n",
      "버섯겜 꼬접하고 로아하러왔습니다.\n",
      "ㄴㄱㄹ 자존심 진짜 존나쎄다 ㅋㅋㅋ\n",
      "현 시점 joat 직업 순위 top3\n",
      "서포터 관련해서 소신발언 한번 하겠습니다.\n",
      "어메이징 로아 쌀먹 근황.jpg\n",
      "“대리충” 박제 쿨 돌았다 에기르에서 얘네 조심해라 얘들아\n",
      "워로드님들께 디붕이가 보내는 편지 &gt;.ㅇ\n",
      "ㅇㅂ) 산악회 새 멤버 관련 평가 및 입장문\n",
      "ㅇㅂ)ㅍㅅ는 원래 논란같은거 신경안씀\n",
      "80+ 딜러님들께 드리는 바드의 인사 편지\n",
      "정보) 왜 하필 천우희 냐는 말이 나오는 이유\n",
      "80+ 서폿님들께 드리는 인사편지\n",
      "길드에서 여자한테 읽씹당했네요...ㅋ\n",
      "ㄴㄱㄹ인지 산악회인지 ㅈ도 관심없으면 개추\n",
      "천우희가 예전에 앨범이 그 분 아닌가\n",
      "ㅇㅂ) 산악회 대타 멤버\n",
      "상재가 왜 설계 미스야ㅋㅋ\n",
      "20대 남성 최대한 징집하는 병무청 ..ㅁㅊ;; ㄷㄷㄷㄷ;;\n",
      "ㅁㅊ; 뷔페에서 교과서로 통하는 청년.jpg\n",
      "ㅁㅊ; 보르노 손가락 여직원 근황\n",
      "이슬비 영업짤\n",
      "보릉보릉 결혼한다는데?\n",
      "ㅁㅊ; 성소수자 범람에 결국 미쳐버린 오버워치 유저들\n",
      "버럭코 키링 무료 나눔 합니다! ( 택배비는 착불임미다..)\n",
      "19) 일본 누나가 알려주는 성인용 단어 ㅗㅜㅑ..ㄷㄷ\n",
      "ㅎㅂ) 이번 수영복 진짜 개 레전드네 캬..\n",
      "ㅇㅎ) 도아가 그대로 들어서\n",
      "첫 연애라 눈치제로인 여자친구ㅋㅋㅋㄷㄷㄷ;;;.jpg\n",
      "유튭 전과자 또 페미 논란.jpg\n",
      "ㅁㅊ; 인싸가 이행해야 할 .jpg\n",
      "사회적 자살 피하기 난이도 최상.gif\n",
      "이딴 패턴 좀 그만 만들어라 제발\n",
      "ㅁㅊ; 쌍둥이 낳고 조기축구 나가는 남편.jpg\n",
      "카멘하면서 이런거 본적 있나?\n",
      "30추 수연글 서머너 계정 산 애임 ㅋㅋㅋㅋㅋ\n",
      "충격적인 \"애낳기좋은나라\"ㅁㅊㄷㄷㄷ;;;.jpg\n",
      "옆동네에서 왔습니다\n",
      "옆집 게임 디렉터 폼 ㅈ되네;\n",
      "방금 자게 올라온 315줄 수연 당사자 바드입니다. 진짜 이해가 안되서 글 올립니다. 한번 보시고 객관적인 말씀 부탁드립니다.\n",
      "난 오히려 1620-&gt;1640 계승이 상재 설계를 더 잘했다 생각하는데?\n",
      "상재20 다하고 계승됐으면 개 지랄 했을거면서ㅋㅋㅋ\n",
      "확실히 로아 신직업 구직업에 차별 존나두긴하네\n",
      "로아 접습니다.\n",
      "칵테일바에 놀러간 실린들\n",
      "사과해줄테니 당사자가 직접 와서 사과받으란 말은 좀 재밋긴 하네요\n",
      "초심 아크패시브 근원의 엘리멘탈\n",
      "화수단 상위1%....jpg\n",
      "눈가루 나무위키 추가됨\n",
      "모코코 기상 지인입니다.\n",
      "아 이클달고 트팟와서 팟 쫑내는 개새끼 좆같네 씨발\n",
      "인파 브커 아크패시브 비교\n",
      "ㅇㅂ) 당사자가 직접 오라는 눈가루님 말씀이 이해 안되는 이유\n",
      "ㅇㅂ) 근데 왜 자꾸 당사자가 직접 오라는거임\n",
      "ㅇㅂ) 김배마 로사단 지원했던 메일 ㅋㅋㅋ\n",
      "체술인파 아크패시브 4티어 '대지 가르기'\n",
      "필보하는데 괘씸해서 박제함 (카단 종남장문장지원)\n",
      "눈가루 해명 관련 날조없이 펙트만 반박해줌 (마지막글)\n",
      "인파 치마 자세 고쳤는데\n",
      "경험치팟 (발탄편)\n",
      "가루님한테 직접 메일로 질문해봤습니다.\n",
      "20만골 편린 먹었다!!!!!!!!!!!!!!!!!!!!!!!!!!!!\n",
      "ㅇㅂ)걍 ㄴㄱㄹ가 존나 똑똑한거 같은데\n",
      "ㅁㅊ;;;;;;;;;싱글벙글 일본의 금쪽이 솔루션.jpg\n",
      "ㅎㅂ)떡 하나 주면 안잡아먹지\n",
      "ㅇㅂ) ㄴㄱㄹ 미션에 대하여\n",
      "ㅇㅂ)눈가루 생방 근황..\n",
      "ㅇㅂ)눈가루 솔직히 이번 논란이 이렇게 장기적으로 불탈 일은 아닌데\n",
      "??? : 꼬우면 내방송 안오면된다\n"
     ]
    }
   ],
   "source": [
    "titlesList = []\n",
    "for board in boardlist:\n",
    "    title = exceptTag(board.select_one(\"td.tit > div > div > a\"))\n",
    "    titlesList.append(title)\n",
    "    print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = np.array(titlesList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got 'korean' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeature_extraction\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CountVectorizer\n\u001b[0;32m      3\u001b[0m count_vect \u001b[38;5;241m=\u001b[39m CountVectorizer(max_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.95\u001b[39m, max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m, min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, stop_words\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkorean\u001b[39m\u001b[38;5;124m'\u001b[39m, ngram_range\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))\n\u001b[1;32m----> 4\u001b[0m feat_vect \u001b[38;5;241m=\u001b[39m \u001b[43mcount_vect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtitles\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCountVectorizer Shape : \u001b[39m\u001b[38;5;124m'\u001b[39m, feat_vect)\n",
      "File \u001b[1;32mc:\\Users\\PC\\CommunityAnalitics\\.venv\\Lib\\site-packages\\sklearn\\base.py:1466\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1461\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1462\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1463\u001b[0m )\n\u001b[0;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1466\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[0;32m   1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\PC\\CommunityAnalitics\\.venv\\Lib\\site-packages\\sklearn\\base.py:666\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    659\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \n\u001b[0;32m    661\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 666\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\PC\\CommunityAnalitics\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'stop_words' parameter of CountVectorizer must be a str among {'english'}, an instance of 'list' or None. Got 'korean' instead."
     ]
    }
   ],
   "source": [
    "df = pd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
